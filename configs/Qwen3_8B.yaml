model:
  path: "Qwen/Qwen3-8B"
  max_new_tokens: 1000
  enable_thinking: false
  torch_dtype: "bfloat16"
  device_map: "cuda"
  attn_implementation: "flash_attention_2"